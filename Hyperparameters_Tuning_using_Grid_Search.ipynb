{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCe4aiDmVcN0"
   },
   "source": [
    "### Tuning Batch Size and Number Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBZ7VJpxNcv9"
   },
   "outputs": [],
   "source": [
    "#set up a grid of hyperparameter values\n",
    "#Retrain the model for each value of the parameter\n",
    "#the one which yields highest accuracy will be selected \n",
    "#We use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model required for KerasClassifier \n",
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(12,input_dim=8, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  #compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt('/content/pima-indians-diabetes.data.csv', delimiter =\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJB_tJTnVg5-",
    "outputId": "23e486c5-4241-4415-f79b-bac873ef0bdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-491742414bed>:6: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "#split into into input (X) and output (Y) variables\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "#Create Model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters for different batch sizes and epochs:\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCcg5kZqcHwo",
    "outputId": "4f69e9ad-cd32-4d97-9078-87991ff336c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.718750 using {'batch_size': 10, 'epochs': 100}\n",
      "0.625000 (0.050126) with {'batch_size': 10, 'epochs': 10}\n",
      "0.674479 (0.004872) with {'batch_size': 10, 'epochs': 50}\n",
      "0.718750 (0.043848) with {'batch_size': 10, 'epochs': 100}\n",
      "0.584635 (0.033502) with {'batch_size': 20, 'epochs': 10}\n",
      "0.684896 (0.009744) with {'batch_size': 20, 'epochs': 50}\n",
      "0.677083 (0.012890) with {'batch_size': 20, 'epochs': 100}\n",
      "0.539062 (0.073079) with {'batch_size': 40, 'epochs': 10}\n",
      "0.652344 (0.030425) with {'batch_size': 40, 'epochs': 50}\n",
      "0.677083 (0.017566) with {'batch_size': 40, 'epochs': 100}\n",
      "0.574219 (0.026107) with {'batch_size': 60, 'epochs': 10}\n",
      "0.645833 (0.041134) with {'batch_size': 60, 'epochs': 50}\n",
      "0.661458 (0.014382) with {'batch_size': 60, 'epochs': 100}\n",
      "0.514323 (0.096442) with {'batch_size': 80, 'epochs': 10}\n",
      "0.640625 (0.008438) with {'batch_size': 80, 'epochs': 50}\n",
      "0.649740 (0.025976) with {'batch_size': 80, 'epochs': 100}\n",
      "0.541667 (0.081085) with {'batch_size': 100, 'epochs': 10}\n",
      "0.562500 (0.030758) with {'batch_size': 100, 'epochs': 50}\n",
      "0.645833 (0.064133) with {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "  # #this code will give me the best batch size with the best number of epochs meaning the best match, Best: 0.718750 using {'batch_size': 10, 'epochs': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUo2QP9uVhEg"
   },
   "source": [
    "### Tuning Optimization algorithms/ optimizers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3rq8LWfVh9-",
    "outputId": "ac688def-8bc6-4c1e-e4fa-deb4820c8c46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-2752aca8fc46>:26: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.723958 using {'optimizer': 'Adam'}\n",
      "0.680990 (0.019225) with: {'optimizer': 'SGD'}\n",
      "0.630208 (0.122714) with: {'optimizer': 'RMSprop'}\n",
      "0.506510 (0.020256) with: {'optimizer': 'Adagrad'}\n",
      "0.621094 (0.034499) with: {'optimizer': 'Adadelta'}\n",
      "0.723958 (0.026557) with: {'optimizer': 'Adam'}\n",
      "0.670573 (0.016053) with: {'optimizer': 'Adamax'}\n",
      "0.703125 (0.022326) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# we have many optimizers such as AdaMax, Adam, RMSprop, Adadelta, AMSGrad, , Adagrad = adaptive Gradient Algorithm, SGD, Nadam\n",
    "#but the most famous ones are SGD = stochastic gradient descent, Adam = adptive moment estimation, RMSprop Root Mean Square Propagation\n",
    "# The optimizer has this objective : Reduce the loss function and help models to make accurate predictions \n",
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model required for KerasClassifier \n",
    "def create_model(optimizer='adam'):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(12,input_dim=8, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  #compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "  return model\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt('/content/pima-indians-diabetes.data.csv', delimiter =\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "#Create Model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n",
    "# define the grid search parameters for different optimizers:\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,Y)\n",
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "  #Best: 0.723958 using {'optimizer': 'Adam'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCevfcNJVipG"
   },
   "source": [
    "### Tuning the Activation Function (in what we call neuron activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-4l5j7UVkks",
    "outputId": "6ae0ec81-1a9e-4f77-b5ff-b8df2061ecfa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-13fc07f8f279>:25: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.730469 using {'activation': 'softplus'}\n",
      "0.653646 (0.009207) with: {'activation': 'softmax'}\n",
      "0.730469 (0.020915) with: {'activation': 'softplus'}\n",
      "0.682292 (0.017566) with: {'activation': 'softsign'}\n",
      "0.718750 (0.029232) with: {'activation': 'relu'}\n",
      "0.673177 (0.032734) with: {'activation': 'tanh'}\n",
      "0.701823 (0.008027) with: {'activation': 'sigmoid'}\n",
      "0.691406 (0.005524) with: {'activation': 'hard_sigmoid'}\n",
      "0.714844 (0.027805) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# we have various and common (non linear) activation functions such as : sigmoid (0 =>1) = logistic function, ReLU (0 =>1 )= rectified linear unit, Softmax, Leaky Relu, Maxout, ELU, tanh ( -1 => 1 )= hyperbolic tangent\n",
    "#Activation Function = parameter we specify when building models, Function applied to the neurons in a layer during prediction \n",
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model required for KerasClassifier \n",
    "def create_model(activation='relu'):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(12,input_dim=8, activation=activation, kernel_initializer='uniform'))\n",
    "  model.add(Dense(1, activation='sigmoid', kernel_initializer='uniform')) # the final layer has always a sigmoid activation function or softmax\n",
    "  #compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt('/content/pima-indians-diabetes.data.csv', delimiter =\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "#Create Model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n",
    "# define the grid search parameters for different activation functions:\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid','linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,Y)\n",
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# the output ==> Best: 0.730469 using {'activation': 'softplus'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYaePrJkVnAy"
   },
   "source": [
    "### Tuning dropout regularization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHywes2GVoYX",
    "outputId": "b2ec1c8f-a3d0-4fc0-b4bd-891535ed4e70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-0d55df30b883>:28: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.740885 using {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "0.725260 (0.011201) with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
      "0.727865 (0.020256) with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
      "0.731771 (0.025780) with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
      "0.731771 (0.027126) with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
      "0.740885 (0.041626) with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
      "0.725260 (0.021236) with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
      "0.733073 (0.019225) with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
      "0.710938 (0.019401) with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
      "0.730469 (0.032369) with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
      "0.726562 (0.008438) with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
      "0.723958 (0.023073) with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
      "0.714844 (0.019401) with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
      "0.729167 (0.019488) with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
      "0.713542 (0.017566) with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
      "0.720052 (0.014731) with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
      "0.712240 (0.019225) with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
      "0.726562 (0.003189) with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
      "0.725260 (0.011201) with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
      "0.722656 (0.024910) with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
      "0.713542 (0.021236) with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
      "0.694010 (0.032106) with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
      "0.717448 (0.010253) with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
      "0.701823 (0.018688) with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
      "0.707031 (0.019401) with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
      "0.716146 (0.019488) with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
      "0.714844 (0.025315) with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
      "0.694010 (0.010253) with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
      "0.701823 (0.014731) with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
      "0.703125 (0.022326) with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
      "0.700521 (0.030314) with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
      "0.690104 (0.021710) with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
      "0.687500 (0.011500) with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
      "0.691406 (0.017758) with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
      "0.694010 (0.008027) with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
      "0.703125 (0.015947) with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
      "0.691406 (0.030425) with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
      "0.679688 (0.036225) with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
      "0.670573 (0.017566) with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
      "0.665365 (0.016367) with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
      "0.671875 (0.019401) with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
      "0.652344 (0.026107) with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
      "0.652344 (0.022999) with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
      "0.653646 (0.027498) with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
      "0.654948 (0.028940) with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
      "0.657552 (0.027498) with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n",
      "0.651042 (0.024774) with: {'dropout_rate': 0.9, 'weight_constraint': 1}\n",
      "0.651042 (0.024774) with: {'dropout_rate': 0.9, 'weight_constraint': 2}\n",
      "0.651042 (0.024774) with: {'dropout_rate': 0.9, 'weight_constraint': 3}\n",
      "0.651042 (0.024774) with: {'dropout_rate': 0.9, 'weight_constraint': 4}\n",
      "0.651042 (0.024774) with: {'dropout_rate': 0.9, 'weight_constraint': 5}\n"
     ]
    }
   ],
   "source": [
    "# applying dropout to a neural net => Drop units/neurons automatically during the training phase  from each layer \n",
    "#randomly remove these units temporarily in order to reduce the computation needs during the training process every epoch, to have  agood performance,  to speed the training phase and to prevent neural networks from overfitting\n",
    "#to reach a good model that captures the underlying logic of the dataset (no capturing the noise of the dataset) and never miss the point (= low loss and high accuracy)\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model required for KerasClassifier \n",
    "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(12,input_dim=8, activation='softplus', kernel_initializer='uniform', kernel_constraint=maxnorm(weight_constraint)))\n",
    "  model.add(Dropout(dropout_rate))\n",
    "  model.add(Dense(1, activation='sigmoid', kernel_initializer='uniform')) # the final layer has always a sigmoid activation function or softmax\n",
    "  #compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt('/content/pima-indians-diabetes.data.csv', delimiter =\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "#Create Model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n",
    "# define the grid search parameters for different dropout rates and weight constraints:\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] # dropout_rate = rate parameter is between 0 and 1 wwhich is a fraction of units we intend to drop\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint = weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,Y)\n",
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# the Best result: 0.740885 using {'dropout_rate': 0.0, 'weight_constraint': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVsYQ_uYVqJG"
   },
   "source": [
    "### Tuning Number of Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xCcO6dF9Vq45",
    "outputId": "a0bab108-73cd-421f-8b3e-5644d0d5cb3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-d7d247e248d5>:30: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.746094 using {'neurons': 25}\n",
      "0.666667 (0.040386) with: {'neurons': 1}\n",
      "0.710938 (0.014616) with: {'neurons': 5}\n",
      "0.730469 (0.024910) with: {'neurons': 10}\n",
      "0.742188 (0.015947) with: {'neurons': 15}\n",
      "0.743490 (0.024360) with: {'neurons': 20}\n",
      "0.746094 (0.019401) with: {'neurons': 25}\n",
      "0.744792 (0.022628) with: {'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "#the number of neurons in a layer is an important parameter to tune. Generally the number of neurons in a layer controls the representational capacity\n",
    "# of the network at least at that point in the topology\n",
    "#Also, generally a large enough single layer network can approximate any other neural network at least in theory\n",
    "# In this example, we will look at tuning the number of neurons in a single hidden layer, we will try values from 1 to 30 in steps of 5\n",
    "#use scikit learn to grid search the number of neurons\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model required for KerasClassifier \n",
    "def create_model(neurons=1):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(neurons,input_dim=8, activation='softplus', kernel_initializer='uniform', kernel_constraint=maxnorm(5)))\n",
    "  model.add(Dropout(0.0))\n",
    "  model.add(Dense(1, activation='sigmoid', kernel_initializer='uniform')) # the final layer has always a sigmoid activation function or softmax\n",
    "  #compile model\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt('/content/pima-indians-diabetes.data.csv', delimiter =\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "#Create Model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n",
    "# define the grid search parameters for different numbers of neurons:\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,Y)\n",
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# the Best result: 0.746094 using {'neurons': 25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBBl4rG1VssO"
   },
   "source": [
    "### learning rate and momentum tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMyjFLhFVtaq",
    "outputId": "3d591a66-38db-465e-cbfa-5d7c31612e15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-f2972644ba8c>:43: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.707031 using {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.690104 (0.011201) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "0.662760 (0.035849) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "0.707031 (0.033146) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.704427 (0.017566) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.695312 (0.032369) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.660156 (0.006379) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "0.622396 (0.055732) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "0.664062 (0.030425) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "0.669271 (0.018688) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "0.649740 (0.026557) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.649740 (0.026557) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "0.427083 (0.134575) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# This is the most important part in the hyperparameter tuning operations \n",
    "# Learning rate : hos fast or how slow is my network learning\n",
    "# how does learning rate impact training \n",
    "# a high learning rate == > the network training will likely be unstable (or diverge entirely), performance of the model will oscillate over training epochs \n",
    "# a low learning rate == > training is more reliable, optimization will take a lot of time\n",
    "\n",
    "# Momentum = a change occured to the weight = weight update, accelerate SGD in the relevant direction \n",
    "# As in learning rate, used when the model is trained by a stochastic gradient descent optimizer\n",
    "\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "# Function to create model required for KerasClassifier \n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(25,input_dim=8, activation='softplus', kernel_initializer='uniform', kernel_constraint=maxnorm(5)))\n",
    "  model.add(Dropout(0.0))\n",
    "  model.add(Dense(1, activation='sigmoid', kernel_initializer='uniform')) # the final layer has always a sigmoid activation function or softmax\n",
    "  #compile model\n",
    "  optimizer= SGD(lr=learn_rate, momentum=momentum)\n",
    "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "  return model\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt('/content/pima-indians-diabetes.data.csv', delimiter =\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "#Create Model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0) # add our chosen batch size and epochs by grid search\n",
    "# define the grid search parameters for different learning rates and momentums:\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum) \n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X,Y)\n",
    "#Summarize Results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# the Best match : Best: 0.707031 using {'learn_rate': 0.001, 'momentum': 0.4}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
